gpodder:
  # Base URL for the gPodder API (e.g. https://gpodder.net)
  base_url: "https://gpodder.net"
  # Initial timestamp to start fetching from (if state is empty). 0 = beginning of time.
  since_timestamp: 0

paths:
  # Directory where downloaded audio files will be stored
  downloads: "data/downloads"
  # Directory where transcripts will be stored
  transcripts: "data/transcripts"
  # Directory where summaries will be stored
  summaries: "data/summaries"
  # Directory where Whisper models will be stored
  models: "data/models"
  # Path to the state file (stores last processed timestamp)
  state_file: "data/state.json"
  # Path to the prompt template file
  prompt_file: "prompt.md"

llm:
  # LLM Provider to use. Options: "gemini", "ollama"
  provider: "gemini"
  
  gemini:
    model: "gemini-3-flash-preview"
    # API key is loaded from .env (GEMINI_API_KEY)
  
  ollama:
    base_url: "http://localhost:11434"
    model: "llama3"

whisper:
  # Root directory of the whisper.cpp installation
  # DO NOT CHANGE if running with Docker (it is installed at /app/whisper.cpp)
  root: "/app/whisper.cpp"
  # Whisper model to use (e.g. "base", "small", "medium", "large-v3")
  model: "medium"
  # Explicit path to the model file.
  # If left empty, it is derived as: {paths.models}/ggml-{model}.bin
  model_path: null
  # Path to the whisper-cli executable. 
  # If left empty, it defaults to {root}/build/bin/whisper-cli
  # DO NOT CHANGE if running with Docker
  bin_path: null
